"""
Script MEJORADO para crear √≠ndice de b√∫squeda TF-IDF local
Incluye: t√≠tulo, href, dc:subject, dc:creator, dc:coverage
"""
import json
import pickle
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

def load_documents():
    with open('clean_with_metadata.json', 'r', encoding='utf-8', errors='ignore') as f:
        docs = json.load(f)
    print(f"üìÇ {len(docs)} documentos cargados desde clean_with_metadata.json")
    return docs

import unicodedata

def normalize_text(text):
    """
    Normalizaci√≥n robusta para b√∫squeda:
    1. Min√∫sculas
    2. Eliminar acentos
    3. Manejo simple de plurales (stemming b√°sico)
    """
    if not isinstance(text, str):
        text = str(text)
    
    # Min√∫sculas y eliminaci√≥n de acentos
    text = text.lower()
    text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')
    
    # Stemming b√°sico para plurales (muy simple para espa√±ol)
    words = text.split()
    stemmed_words = []
    for word in words:
        # Si termina en 'es' (√°rboles -> √°rbol, canciones -> cancion)
        if word.endswith('es') and len(word) > 4:
            word = word[:-2]
        # Si termina en 's' (casas -> casa)
        elif word.endswith('s') and len(word) > 3 and not word.endswith('ss'):
            word = word[:-1]
        stemmed_words.append(word)
    
    return ' '.join(stemmed_words)

def create_search_index(documents):
    """Crea √≠ndice TF-IDF COMPLETO para b√∫squeda"""
    print("üîÑ Creando √≠ndice de b√∫squeda TF-IDF MEJORADO (con stemming)...")
    
    TEXT_FIELDS = ["dc:title", "dc:creator", "dc:subject", "dc:coverage"]
    
    texts = []
    for doc in documents:
        parts = []
        
        # Usar TEXT_FIELDS definidos por el usuario
        for field in TEXT_FIELDS:
            val = doc.get(field, '')
            if isinstance(val, list):
                parts.extend([str(v) for v in val])
            elif val:
                parts.append(str(val))
        
        # Unir todo y NORMALIZAR
        full_text = ' '.join(str(p) for p in parts if p)
        # Aqu√≠ aplicamos la normalizaci√≥n para que el √≠ndice contenga t√©rminos normalizados
        normalized_text = normalize_text(full_text)
        texts.append(normalized_text)
    
    # Crear vectorizador TF-IDF con configuraci√≥n optimizada
    vectorizer = TfidfVectorizer(
        max_features=15000,      # M√°s vocabulario
        ngram_range=(1, 3),      # Hasta trigramas para frases como "Consejo de Gabinete"
        stop_words=None,         # Mantener todas las palabras
        min_df=1,                # Incluir t√©rminos raros
        max_df=0.90,             # Excluir t√©rminos muy comunes
        token_pattern=r'(?u)\b[\w-]+\b',  # Incluir palabras con guiones
    )
    
    tfidf_matrix = vectorizer.fit_transform(texts)
    
    print(f"‚úÖ √çndice creado: {tfidf_matrix.shape[0]} docs x {tfidf_matrix.shape[1]} t√©rminos")
    
    return {
        'vectorizer': vectorizer,
        'matrix': tfidf_matrix,
        'texts': texts
    }

def save_index(index_data):
    with open('search_index.pkl', 'wb') as f:
        pickle.dump(index_data, f)
    print("üíæ √çndice guardado en search_index.pkl")

if __name__ == "__main__":
    print("=" * 50)
    print("üîç CREACI√ìN DE √çNDICE TF-IDF MEJORADO")
    print("=" * 50)
    
    documents = load_documents()
    index = create_search_index(documents)
    save_index(index)
    
    print("=" * 50)
    print("‚úÖ √çNDICE LISTO - Incluye t√≠tulo, href, subjects,")
    print("   creators, coverage y dates")
    print("=" * 50)
